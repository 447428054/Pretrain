# 预训练数据处理流程

1. 每个文件中，一个sentence占一行，不同document之间加一个空行分割
[['有', '人', '知', '道', '叫', '什', '么', '名', '字', '吗', '[UNK]', '？'], ['有', '人', '知', '道', '名', '字', '吗']]

2. 从一个文档中连续的获得文本，直到达到最大长度。如果是从下一个文档中获得，那么加上一个分隔符.将长度限制修改了，因为lcqmc句子都偏短
['有', '人', '知', '道', '叫', '什', '么', '名', '字', '吗', '[UNK]', '？', '有', '人', '知', '道', '名', '字', '吗']

3. 对于获取之后的文本，进行全词分词： 判断每个字符起始长度3以内的，是否在分词里面，在的话添加##标记
['有', '##人', '知', '##道', '叫', '什', '##么', '名', '##字', '吗', '[UNK]', '？', '有', '##人', '知', '##道', '名', '##字', '吗']

4. 对获得的token序列，进行掩码:返回 掩码结果，掩码的位置，掩码的标签
['[CLS]', '有', '人', '知', '道', '叫', '什', '么', '名', '字', '吗', '[UNK]', '[MASK]', '有', '人', '[MASK]', '[MASK]', '名', '字', '吗', '[SEP]']
[12, 15, 16]
['？', '知', '##道']


run_pretraining.py需要注释TPU的应用
# tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver( # TODO
#       tpu=FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)

